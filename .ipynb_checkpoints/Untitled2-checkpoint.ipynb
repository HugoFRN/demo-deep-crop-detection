{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How-To: Use Deep Learning models with eo-learn\n",
    "This notebook show how deep learning algorithms can be used inside an eo-learn workflow to perform a crop type identification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "### Requirements\n",
    "\n",
    "* This notebook doesn't describe how to train a neural network but rather how it is possible to integrate a already trained one inside a workflow to perform some kind of processing. This exemple use a RefineNet[1] model that was trained to produce semantic segmentation of crop type from Sentinel-2 images.\n",
    "\n",
    "\n",
    "* In order to run the example you’ll need a configured Sentinel Hub account (see the [configuration instructions](https://sentinelhub-py.readthedocs.io/en/latest/configure.html) if necessary)\n",
    "\n",
    "\n",
    "* This neural network was trained using the [Keras](https://keras.io/) framework version 2.1.3 with [tensorflow](https://www.tensorflow.org/) version 1.4.1 and need to be installed. You can do so by using: \n",
    "``` shell\n",
    "pip install keras==2.1.3\n",
    "pip install tensorflow==1.4.1\n",
    "pip install tensorflow-gpu==1.4.1 #for GPU support\n",
    "``` \n",
    "\n",
    "### Overview\n",
    "This notebook is divided in 2 sections:\n",
    " 1. Defining the required EO-task \n",
    " 2. visualizing the results\n",
    "\n",
    "[1]: Lin, G., Milan, A., Shen, C., & Reid, I. (2017, juillet). RefineNet: Multi-path Refinement Networks for High-Resolution Semantic Segmentation. 5168‑5177. https://doi.org/10.1109/CVPR.2017.549"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the required EO-task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task we will use a shapefile containing all the crop fields in australia during 2016 as an exemple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T12:28:58.673877Z",
     "start_time": "2019-11-29T12:27:43.407654Z"
    }
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "austria_lpis_2016 = gpd.read_file('./Austria_LPIS/2016/INSPIRE_SCHLAEGE_POLY_2016_reproj.shp')\n",
    "austria_lpis_2016.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T12:47:38.884488Z",
     "start_time": "2019-11-28T12:47:38.879208Z"
    }
   },
   "source": [
    "Each entry from the shapefile contain a polygon defining a field in austria with its crop type within the **SNAR_BEZEI** field.\n",
    "\n",
    "We want to see if our model can predict the correct crop type of these field based on Sentinel-2 data. We need some EO-Task that extract location information from the polygon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T12:29:23.720085Z",
     "start_time": "2019-11-29T12:29:23.303934Z"
    }
   },
   "outputs": [],
   "source": [
    "from eolearn.core import EOPatch, FeatureType, EOTask\n",
    "from sentinelhub.areas import BBox\n",
    "from sentinelhub.constants import CRS\n",
    "\n",
    "class InitEOPatch(EOTask):\n",
    "    def __init__(self, time_interval):\n",
    "        # Time interval for the Sentinel-2 data\n",
    "        self.time_interval = time_interval\n",
    "    \n",
    "    def execute(self, polygon):\n",
    "        geometry = polygon.buffer(0.005)\n",
    "        \n",
    "        # Get Bounding box of the polygon\n",
    "        envelope = geometry.envelope.geometry[geometry.envelope.geometry.keys().tolist()[0]]\n",
    "        bbox = BBox(envelope, crs=CRS.WGS84)\n",
    "        \n",
    "        # Store the data inside the EOPatch\n",
    "        eopatch = EOPatch()\n",
    "        eopatch.vector_timeless['polygon'] = polygon\n",
    "#         eopatch.vector_timeless['polygon'] = gpd.GeoDataFrame(polygon, crs={'init' :'WGS84'})\n",
    "        eopatch.meta_info['time_interval'] = self.time_interval\n",
    "        eopatch.bbox = bbox\n",
    "        return eopatch\n",
    "\n",
    "# Initialise the task for data taken suring the 2016 summer\n",
    "init_task = InitEOPatch(('2016-05-01', '2016-08-31'))\n",
    "\n",
    "#Display an example from one polygon:\n",
    "example_patch = init_task.execute(austria_lpis_2016[0:1])\n",
    "display(example_patch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to download Sentinel-2 data in the time interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T12:29:38.574132Z",
     "start_time": "2019-11-29T12:29:36.049955Z"
    }
   },
   "outputs": [],
   "source": [
    "from eolearn.io import S2L1CWCSInput\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import rasterio.features\n",
    "import math\n",
    "\n",
    "# Normalise the image array for better colour visualisation\n",
    "def normalize_percentile(image, percentile):\n",
    "    if (np.percentile(image, 100 - percentile) - np.percentile(image, percentile)) != 0:\n",
    "        return np.clip((image - np.percentile(image, percentile)) /\n",
    "                       (np.percentile(image, 100 - percentile) -\n",
    "                        np.percentile(image, percentile))*255, 0, 255).astype(np.uint8)\n",
    "    else:\n",
    "        return np.zeros_like(image)\n",
    "\n",
    "# Download the images\n",
    "input_task = S2L1CWCSInput(layer='BANDS-S2-L1C', resx='10m', resy='10m', maxcc=0.2)\n",
    "example_patch = input_task.execute(example_patch)\n",
    "\n",
    "# Get the images array\n",
    "images = example_patch.data['BANDS-S2-L1C'][:,:,:,[3,2,1]]\n",
    "\n",
    "# Overlay the polygon by rasterising it:\n",
    "bbox_poly = example_patch.bbox.get_geometry()\n",
    "geomtrie = example_patch.vector_timeless['polygon']\n",
    "filtered_data = geomtrie[geomtrie.intersects(bbox_poly)].copy(deep=True)\n",
    "filtered_data.geometry = filtered_data.geometry.intersection(bbox_poly).exterior\n",
    "height, width = example_patch.get_spatial_dimension(FeatureType.MASK, 'IS_DATA')\n",
    "data_transform = rasterio.transform.from_bounds(*example_patch.bbox, width=width, height=height)\n",
    "raster = np.zeros_like(example_patch.mask['IS_DATA'][0].squeeze()).astype(np.uint8)\n",
    "rasterio.features.rasterize([(filtered_data.cascaded_union, 255)], out=raster, transform=data_transform, dtype=np.uint8)\n",
    "\n",
    "# Display all images\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "for i, image in enumerate(images):\n",
    "    ax = plt.subplot(math.ceil(images.shape[0]/3), 3, i + 1)\n",
    "    image = normalize_percentile(image, 2)\n",
    "    # Display the polygon in red\n",
    "    image[raster==255] = [255, 0, 0]\n",
    "    plt.imshow(image)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    ax.set_aspect(\"auto\")\n",
    "fig.subplots_adjust(wspace=0.01, hspace=0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T13:28:33.667273Z",
     "start_time": "2019-11-28T13:28:33.660403Z"
    }
   },
   "source": [
    "We can now run our deep learning model on Sentinel-2 data:\n",
    "\n",
    "We need to setup a few EO-Task to prepare the data and run the model inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T12:29:46.643795Z",
     "start_time": "2019-11-29T12:29:42.311948Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Our model was trained on uint16  data:\n",
    "class Float32ToUINT16(EOTask):\n",
    "    def __init__(self, feature):\n",
    "        self.feature = self._parse_features(feature, new_names=True,\n",
    "                                            default_feature_type=FeatureType.DATA,\n",
    "                                            rename_function='{}-UINT16'.format)\n",
    "    \n",
    "    def execute(self, eopatch):\n",
    "        feature_type, feature_name, new_feature_name = next(self.feature(eopatch))\n",
    "        eopatch[feature_type][new_feature_name] = eopatch[feature_type][feature_name] * 10000\n",
    "        eopatch[feature_type][new_feature_name] = eopatch[feature_type][new_feature_name].astype(np.uint16)\n",
    "        return eopatch\n",
    "\n",
    "# See deep_eval.py for more detail of our implementation\n",
    "from deep_eval import DeepEval\n",
    "\n",
    "model_path='/media/data_deep/DATA_H2020_PERSEN/TEST_DATASET/output/trained/CropGroup_small_classes/20190320_1345_unet_unet_relubn__resume_CropGroup_small_classes_softmax_099_crop_sgd/'\n",
    "cast_task = Float32ToUINT16((FeatureType.DATA, 'BANDS-S2-L1C', 'BANDS-S2-L1C-UINT16'))\n",
    "eval_task = DeepEval((FeatureType.DATA, 'BANDS-S2-L1C-UINT16', 'MODEL_EVAL'), model_path, gpu_mode=True)\n",
    "\n",
    "example_patch = cast_task.execute(example_patch)\n",
    "example_patch = eval_task.execute(example_patch)\n",
    "display(example_patch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T12:29:49.774991Z",
     "start_time": "2019-11-29T12:29:49.455435Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the evaluations\n",
    "masks = example_patch.mask['MODEL_EVAL']\n",
    "\n",
    "# Display all evaluations\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "for i, mask in enumerate(masks):\n",
    "    ax = plt.subplot(math.ceil(masks.shape[0]/3), 3, i + 1)\n",
    "    # Display the polygon in red\n",
    "    mask[raster==255] = [255, 0, 0]\n",
    "    plt.imshow(mask)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    ax.set_aspect(\"auto\")\n",
    "fig.subplots_adjust(wspace=0.01, hspace=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now average all the predictions in a single one to extract the crop type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T12:30:18.564995Z",
     "start_time": "2019-11-29T12:30:18.476019Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from deep_eval import class_matrix_to_bgr_mask, softmax_to_class_matrix\n",
    "\n",
    "# We extract the average prediction ignoring background and clouds\n",
    "class SumPrediction(EOTask):\n",
    "    def __init__(self, feature, classes_to_ignore=np.asarray([]), threshold=0.5):\n",
    "        self.feature = self._parse_features(feature, new_names=True,\n",
    "                                            default_feature_type=FeatureType.DATA,\n",
    "                                            rename_function='{}_ALL-TIME'.format)\n",
    "        self.classes_to_ignore = classes_to_ignore\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def execute(self, eopatch):\n",
    "        feature_type, feature_name, new_feature_name = next(self.feature(eopatch))\n",
    "        prediction = eopatch[feature_type][feature_name].sum(axis=0) / len(eopatch.timestamp)\n",
    "        prediction = softmax_to_class_matrix(prediction,\n",
    "                                classes_to_ignore=self.classes_to_ignore,\n",
    "                                threshold=self.threshold)\n",
    "        prediction = class_matrix_to_bgr_mask(prediction, eopatch.meta_info['class_rgb'], in_place=True)\n",
    "        eopatch.mask_timeless[new_feature_name] = prediction\n",
    "        return eopatch\n",
    "\n",
    "prediction_task = SumPrediction((FeatureType.MASK, 'MODEL_EVAL_PROBA', 'ALL-TIME_PREDICTION'), classes_to_ignore=np.asarray([1]), threshold=0.5)\n",
    "example_patch = prediction_task.execute(example_patch)\n",
    "\n",
    "prediction = example_patch.mask_timeless['ALL-TIME_PREDICTION']\n",
    "prediction[raster==255] = [255, 0, 0]\n",
    "plt.figure()\n",
    "plt.imshow(prediction)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "ax.set_aspect(\"auto\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T15:40:44.831976Z",
     "start_time": "2019-11-28T15:40:44.824136Z"
    }
   },
   "source": [
    "This prediction can now be used to extract the crop type most present inside the polygon and finally get a prediction to compare with the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T12:30:22.101701Z",
     "start_time": "2019-11-29T12:30:22.017412Z"
    }
   },
   "outputs": [],
   "source": [
    "from deep_eval import bgr_mask_to_class_matrix\n",
    "\n",
    "class Rasterize(EOTask):\n",
    "    def __init__(self, polygon_feature, feature, raster_shape, raster_value):\n",
    "        self.raster_value = raster_value\n",
    "        self.raster_shape_type, self.raster_shape_name = raster_shape\n",
    "        self.feature_type, self.feature_name = feature\n",
    "        self.polygon_type, self.polygon_name = polygon_feature\n",
    "        \n",
    "    def execute(self, eopatch):\n",
    "        shape = eopatch[self.raster_shape_type][self.raster_shape_name][0,:,:,0].shape\n",
    "        raster_filled = rasterio.features.rasterize(eopatch[self.polygon_type][self.polygon_name].geometry, out_shape=shape, transform=rasterio.transform.from_bounds(*eopatch.bbox, height=shape[0], width=shape[1]), default_value=self.raster_value)\n",
    "        eopatch[self.feature_type][self.feature_name] = np.expand_dims(raster_filled, axis=-1)\n",
    "        return eopatch\n",
    "    \n",
    "class ExtractField(EOTask):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def execute(self, eopatch):\n",
    "        eval_in_shape = bgr_mask_to_class_matrix(eopatch.mask_timeless['ALL-TIME_PREDICTION'], eopatch.meta_info['class_rgb'])[np.all(eopatch.mask_timeless['CLASS_MASK'] == 255, axis=-1)]\n",
    "        if len(eval_in_shape) > 0 and (eval_in_shape.shape[0] > 1):\n",
    "                if eval_in_shape.shape[-1] == 1:\n",
    "                    eval_in_shape = eval_in_shape.squeeze()\n",
    "                found = np.unique(eval_in_shape, axis=0, return_counts=True)\n",
    "                if len(found) == 2 and len(found[0]) > 0 and len(found[1]) > 0:\n",
    "                    most_present_class = found[0][np.argmax(found[1])]\n",
    "                    eopatch.label_timeless['LABEL_NUMBER'] = np.array([most_present_class])\n",
    "                    eopatch.meta_info['FOUND_LABEL'] = [eopatch.meta_info['class_labels'][most_present_class]]\n",
    "        return eopatch\n",
    "    \n",
    "rasterize_gt_task = Rasterize((FeatureType.VECTOR_TIMELESS, 'polygon'), (FeatureType.MASK_TIMELESS, 'CLASS_MASK'), (FeatureType.MASK, 'IS_DATA'), 255)\n",
    "example_patch = rasterize_gt_task.execute(example_patch)\n",
    "\n",
    "extract_field_task = ExtractField()\n",
    "example_patch = extract_field_task.execute(example_patch)\n",
    "display(example_patch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T16:43:40.595709Z",
     "start_time": "2019-11-28T16:43:40.589140Z"
    }
   },
   "source": [
    "We can see in the **FOUND_LABEL** field that the predicted crop type is **maize**.\n",
    "\n",
    "## Visualisation\n",
    "\n",
    "Finally we can join all thoses tasks inside a workflow to easily use all the previously defined task on any polygon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T12:30:27.900124Z",
     "start_time": "2019-11-29T12:30:25.022691Z"
    }
   },
   "outputs": [],
   "source": [
    "from eolearn.core import LinearWorkflow\n",
    "\n",
    "workflow = LinearWorkflow(init_task,\n",
    "                           input_task,\n",
    "                           cast_task, \n",
    "                           eval_task,\n",
    "                           prediction_task,\n",
    "                           rasterize_gt_task,\n",
    "                           extract_field_task) \n",
    "\n",
    "polygon = austria_lpis_2016[5:6]\n",
    "patch = workflow.execute({\n",
    "            init_task: {'polygon': polygon}\n",
    "        })\n",
    "for x in patch.keys():\n",
    "    patch = patch[x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualise the results here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T12:30:31.602461Z",
     "start_time": "2019-11-29T12:30:30.817320Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Overlay the polygon by rasterising it:\n",
    "bbox_poly = patch.bbox.get_geometry()\n",
    "geomtrie = patch.vector_timeless['polygon']\n",
    "filtered_data = geomtrie[geomtrie.intersects(bbox_poly)].copy(deep=True)\n",
    "filtered_data.geometry = filtered_data.geometry.intersection(bbox_poly).exterior\n",
    "height, width = patch.get_spatial_dimension(FeatureType.MASK, 'IS_DATA')\n",
    "data_transform = rasterio.transform.from_bounds(*patch.bbox, width=width, height=height)\n",
    "raster = np.zeros_like(patch.mask['IS_DATA'][0].squeeze()).astype(np.uint8)\n",
    "rasterio.features.rasterize([(filtered_data.cascaded_union, 255)], out=raster, transform=data_transform, dtype=np.uint8)\n",
    "\n",
    "nb_images = patch.data['BANDS-S2-L1C'].shape[0]\n",
    "for i in range(nb_images):\n",
    "    plt.figure(figsize=(8,3))\n",
    "    ax = plt.subplot(1, 2,1)\n",
    "    image = normalize_percentile(patch.data['BANDS-S2-L1C'][i,:,:,[3,2,1]].transpose((1,2,0)), 2)\n",
    "    image[raster == 255] = [255, 0, 0]\n",
    "    ax.imshow(image)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "#     ax.set_aspect(\"auto\")\n",
    "\n",
    "    ax = plt.subplot(1, 2, 2)\n",
    "    mask = patch.mask['MODEL_EVAL'][i]\n",
    "    mask[raster == 255] = [255, 0, 0]\n",
    "    ax.imshow(mask)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "#     ax.set_aspect(\"auto\")\n",
    "fig.subplots_adjust(wspace=0.01, hspace=0.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
